{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log in to huggingface in command line\n",
    "# huggingface-cli login in python scrips directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saraf\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"openai/openai_humaneval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task_id': 'HumanEval/0', 'prompt': 'from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n', 'canonical_solution': '    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n\\n    return False\\n', 'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\\n    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\\n\\n\", 'entry_point': 'has_close_elements'}\n"
     ]
    }
   ],
   "source": [
    "#inspect dataset\n",
    "print(dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to excel\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset[\"test\"])\n",
    "df.to_excel('HumanEval.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base directories\n",
    "base_dir = \"processed_dataset\"\n",
    "dirs = {\n",
    "    \"functions\": os.path.join(base_dir, \"functions\"),\n",
    "    \"tests\": os.path.join(base_dir, \"tests\"),\n",
    "    \"descriptions\": os.path.join(base_dir, \"descriptions\"),\n",
    "}\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_path in dirs.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add __init__.py files to tests and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process All Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current issues:\n",
    "1. loop blocks are not indented correctly in the test files\n",
    "2. bit writing all functions when there's more than one function (helper functions).\n",
    "3. some tests are empty (example: 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset[\"test\"])):\n",
    "    # 0. Process a single instance\n",
    "    dataset_instance = dataset[\"test\"][i]\n",
    "    task_id = dataset_instance[\"task_id\"].replace(\"/\", \"_\")  # Ensure valid filename\n",
    "    function_name = dataset_instance[\"entry_point\"]\n",
    "\n",
    "    # 1. Extract the function definition and docstring\n",
    "    prompt = dataset_instance[\"prompt\"]\n",
    "    function_definition = re.findall(r\"def [^\\n]*\", prompt)[0]  # Extract the function definition (first line)\n",
    "    docstring = re.findall(r'\"\"\"(.*?)\"\"\"', prompt, re.DOTALL)  # Extract docstring between triple quotes\n",
    "\n",
    "    # 2. Save the function implementation (without the docstring)\n",
    "    func_file_path = os.path.join(dirs[\"functions\"], f\"{task_id}.py\")\n",
    "    with open(func_file_path, \"w\") as func_file:\n",
    "        #check for imports and add if present\n",
    "        prompt_lines = dataset_instance[\"prompt\"].splitlines()\n",
    "        first_line = prompt_lines[0].strip()\n",
    "        if(first_line.startswith(\"from\") | first_line.startswith(\"import\")):\n",
    "            func_file.write(first_line + \"\\n\")\n",
    "\n",
    "        #write cannonical solution\n",
    "        func_file.write(function_definition + \"\\n\")\n",
    "        func_file.write(dataset_instance[\"canonical_solution\"] + \"\\n\")\n",
    "\n",
    "    # 3. Save the test cases\n",
    "    # Convert check(candidate) to be compatible with pytest\n",
    "    test_file_path = os.path.join(dirs[\"tests\"], f\"test_{task_id}.py\")  # Use task_id for the filename\n",
    "    with open(test_file_path, \"w\") as test_file:\n",
    "        # Import the function from the corresponding module\n",
    "        test_file.write(f\"from functions.{task_id} import {function_name}\\n\\n\")\n",
    "    \n",
    "        # Write the test function\n",
    "        test_file.write(f\"def test_{task_id}():\\n\")\n",
    "        test_lines = dataset_instance[\"test\"].splitlines()\n",
    "        for line in test_lines[9:]:  # Skip the `def check(candidate):` line\n",
    "            l = line.strip()\n",
    "            mod = l.replace(\"candidate\", function_name)\n",
    "            test_file.write(f\"  {mod}\\n\")  # Properly indent the lines\n",
    "    \n",
    "    # 4. Save the description (docstring)\n",
    "    desc_file_path = os.path.join(dirs[\"descriptions\"], f\"{task_id}_description.txt\")\n",
    "    with open(desc_file_path, \"w\",encoding=\"utf-8\") as desc_file:\n",
    "        desc_file.write(f\"Task ID: {dataset_instance['task_id']}\\n\")\n",
    "        if docstring:\n",
    "            desc_file.write(f\"Description:\\n{docstring[0].strip()}\\n\")  # Clean docstring without extra spaces\n",
    "        else:\n",
    "            desc_file.write(\"No description provided.\\n\")\n",
    "        desc_file.write(f\"Entry Point: {dataset_instance['entry_point']}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Test Coverage\n",
    "\n",
    "1. run python -m pytest --cov=functions --cov-report=term-missing > coverage_report.txt\n",
    "2. run compute_cov.sh script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process One Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_close_elements\n",
      "HumanEval_0\n"
     ]
    }
   ],
   "source": [
    "dataset_instance = dataset[\"test\"][0]\n",
    "\n",
    "# Process a single instance\n",
    "task_id = dataset_instance[\"task_id\"].replace(\"/\", \"_\")  # Ensure valid filename\n",
    "function_name = dataset_instance[\"entry_point\"]\n",
    "\n",
    "print(function_name)\n",
    "print(task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract the function definition and docstring\n",
    "prompt = dataset_instance[\"prompt\"]\n",
    "function_definition = re.findall(r\"def [^\\n]*\", prompt)[0]  # Extract the function definition (first line)\n",
    "docstring = re.findall(r'\"\"\"(.*?)\"\"\"', prompt, re.DOTALL)  # Extract docstring between triple quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 Current Issues:\n",
    "1. (RESOLVED) not writing in function imports when needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Save the function implementation (without the docstring)\n",
    "func_file_path = os.path.join(dirs[\"functions\"], f\"{task_id}.py\")\n",
    "with open(func_file_path, \"w\") as func_file:\n",
    "    #check for imports and add if present\n",
    "    prompt_lines = dataset_instance[\"prompt\"].splitlines()\n",
    "    first_line = prompt_lines[0].strip()\n",
    "    if(first_line.startswith(\"from\") | first_line.startswith(\"import\")):\n",
    "        func_file.write(first_line + \"\\n\")\n",
    "\n",
    "    #write cannonical solution\n",
    "    func_file.write(function_definition + \"\\n\")\n",
    "    func_file.write(dataset_instance[\"canonical_solution\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3 Current Issues: \n",
    "1. not sure if writing in test case file in the most efficient way (ie: guessing on which line to start, changing the test to a different format than given in HumanEval)\n",
    "2. not sure if current way of writing test case file is compilable with proper path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the test cases\n",
    "# Convert check(candidate) to be compatible with pytest\n",
    "test_file_path = os.path.join(dirs[\"tests\"], f\"test_{task_id}.py\")  # Use task_id for the filename\n",
    "with open(test_file_path, \"w\") as test_file:\n",
    "    # Import the function from the corresponding module\n",
    "    test_file.write(f\"from functions.{task_id} import {function_name}\\n\\n\")\n",
    "    \n",
    "    # Write the test function\n",
    "    test_file.write(f\"def test_{task_id}():\\n\")\n",
    "    test_lines = dataset_instance[\"test\"].splitlines()\n",
    "    for line in test_lines[9:]:  # Skip the `def check(candidate):` line\n",
    "        l = line.strip()\n",
    "        mod = l.replace(\"candidate\", function_name)\n",
    "        test_file.write(f\"  {mod}\\n\")  # Properly indent the lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Save the description (docstring)\n",
    "desc_file_path = os.path.join(dirs[\"descriptions\"], f\"{task_id}_description.txt\")\n",
    "with open(desc_file_path, \"w\") as desc_file:\n",
    "    desc_file.write(f\"Task ID: {dataset_instance['task_id']}\\n\")\n",
    "    if docstring:\n",
    "        desc_file.write(f\"Description:\\n{docstring[0].strip()}\\n\")  # Clean docstring without extra spaces\n",
    "    else:\n",
    "        desc_file.write(\"No description provided.\\n\")\n",
    "    desc_file.write(f\"Entry Point: {dataset_instance['entry_point']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
